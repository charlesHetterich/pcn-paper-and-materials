{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.tensorboard as tb\n",
    "\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from datasets import load_data, MNIST\n",
    "import pcn_kernels\n",
    "\n",
    "\n",
    "class pcnpass(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(c: torch.Tensor, l: torch.Tensor, lnext: torch.Tensor, b: torch.Tensor):\n",
    "        output = pcn_kernels.forward(c, l, lnext, b)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, _):\n",
    "        c, l, lnext, b = inputs\n",
    "        ctx.save_for_backward(c, l, lnext, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_cnext):\n",
    "        c, l, lnext, b = ctx.saved_tensors\n",
    "        grad_c, grad_l, grad_lnext, grad_b = pcn_kernels.backward(grad_cnext.contiguous(), c, l, lnext, b)\n",
    "        return grad_c, grad_l, grad_lnext, grad_b\n",
    "\n",
    "class PCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: list[int],\n",
    "        dimensions: int = 20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.weight_transform = tri(0.1, 1)\n",
    "        if len(layers) < 2:\n",
    "            raise ValueError(\"At least 2 layers are required\")\n",
    "\n",
    "        self.layers = nn.ParameterList(\n",
    "            [nn.Parameter(torch.rand(l, dimensions) * 2 - 1) for l in layers]\n",
    "        )\n",
    "        self.layers_bias = nn.ParameterList(\n",
    "            [nn.Parameter((torch.rand(l) * 2 - 1) * 0.1) for l in layers]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = x\n",
    "        for i, (l, lnext) in enumerate(zip(self.layers, self.layers[1:])):\n",
    "            z = pcnpass.apply(z, l, lnext, self.layers_bias[i + 1])\n",
    "            if i < len(self.layers) - 2:\n",
    "                z = torch.relu(z)\n",
    "        return z\n",
    "\n",
    "def tri(period: float, amplitude: float):\n",
    "    \"\"\"\n",
    "    triangle wave function centered around 0 with period and amplitude\n",
    "    \"\"\"\n",
    "\n",
    "    def triangle_wave_transform(x: torch.Tensor):\n",
    "        # using sigal\n",
    "        return (amplitude / period) * (\n",
    "            (period - abs(x % (2 * period) - (1 * period)) - period / 2)\n",
    "        )\n",
    "\n",
    "    return triangle_wave_transform\n",
    "\n",
    "class PCN_Old(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: list[int],\n",
    "        dimensions: int = 20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.weight_transform = tri(0.1, 1)\n",
    "        if len(layers) < 2:\n",
    "            raise ValueError(\"At least 2 layers are required\")\n",
    "\n",
    "        self.layers = nn.ParameterList(\n",
    "            [nn.Parameter(torch.rand(l, dimensions) * 2 - 1) for l in layers]\n",
    "        )\n",
    "        self.layers_bias = nn.ParameterList(\n",
    "            [nn.Parameter((torch.rand(l, 1) * 2 - 1) * 0.1) for l in layers]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = x\n",
    "        for i, (l, lnext) in enumerate(zip(self.layers, self.layers[1:])):\n",
    "            z = (\n",
    "                z @ (self.weight_transform(torch.cdist(l, lnext)) / np.sqrt(l.shape[0]))\n",
    "            ) + self.layers_bias[i + 1].T\n",
    "            if i < len(self.layers) - 2:\n",
    "                z = torch.relu(z)\n",
    "        return z\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        hidden: list[int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        c = n_in\n",
    "        L = []\n",
    "        for l in hidden:\n",
    "            L.append(nn.Linear(c, l))\n",
    "            L.append(nn.ReLU())\n",
    "            c = l\n",
    "\n",
    "        L.append(nn.Linear(l, n_out))\n",
    "        self.net = nn.Sequential(*L)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class pcnSGD(torch.optim.Optimizer):\n",
    "    def __init__(self, model: nn.Module, lr=1e-3, opp=None):\n",
    "        self.opp = opp\n",
    "        self.model = model\n",
    "        defaults = dict(lr=lr)\n",
    "        super(pcnSGD, self).__init__(model.parameters(), defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        # for param in \n",
    "\n",
    "        for group in self.param_groups:\n",
    "            positions = group[\"params\"][:int(len(group[\"params\"])/2)]\n",
    "            biases = group[\"params\"][int(len(group[\"params\"])/2):]\n",
    "\n",
    "            for pos in positions:\n",
    "                if pos.grad is None:\n",
    "                    continue\n",
    "                grad = pos.grad.data\n",
    "                # apply custom gradient descent update\n",
    "                \n",
    "                if self.opp == None:\n",
    "                    pos.data.add_(-group[\"lr\"] * (pos.shape[0]), grad)\n",
    "                elif self.opp == \"log\":\n",
    "                    pos.data.add_(-group[\"lr\"] * (pos.shape[0] / np.log2(pos.shape[0])), grad)\n",
    "                elif self.opp == \"sqrt\":\n",
    "                    pos.data.add_(-group[\"lr\"] * (np.sqrt(pos.shape[0])), grad)\n",
    "\n",
    "\n",
    "            for bias in biases:\n",
    "                if bias.grad is None:\n",
    "                    continue\n",
    "                grad = bias.grad.data\n",
    "                # apply custom gradient descent update\n",
    "                bias.data.add_(-group[\"lr\"] * 1e5, grad)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Data\n",
    "\n",
    "train = load_data(MNIST(\"train\"))\n",
    "test = load_data(MNIST(\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"log_dir\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = \"log_dir\"\n",
    "learn_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "device = gpu if torch.cuda.is_available() else cpu\n",
    "\n",
    "models: list[tuple[str, nn.Module]] = [\n",
    "    (\"pcn512_16D_Old\", PCN_Old([28 * 28, 512, 10], dimensions=16).to(device)),\n",
    "    (\"pcn512_16D_New\", PCN([28 * 28, 512, 10], dimensions=16).to(device)),\n",
    "    (\"fcn_100_100_100\", FCN(28 * 28, 10, [100, 100, 100]).to(device)),\n",
    "]\n",
    "\n",
    "loggers = [tb.SummaryWriter(log_dir + \"/\" + name) for name, _ in models]\n",
    "optimizers = [\n",
    "    pcnSGD(models[0][1], lr=0.00002, opp=\"log\"),\n",
    "    pcnSGD(models[1][1], lr=0.00002, opp=\"log\"),\n",
    "    torch.optim.SGD(models[2][1].parameters(), lr=0.01),\n",
    "]\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (x, y) in enumerate(train):\n",
    "        x = x.float().to(device)\n",
    "        y = y.to(device)\n",
    "        for (_, model), optimzer, logger in zip(models, optimizers, loggers):\n",
    "            pred = model(x).float()\n",
    "            l = loss(pred, y)\n",
    "\n",
    "            l.backward()\n",
    "\n",
    "            optimzer.step()\n",
    "            optimzer.zero_grad()\n",
    "\n",
    "            logger.add_scalar(\"loss\", l, step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (_, model), logger in zip(models, loggers):\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (x, y) in enumerate(test):\n",
    "                x = x.float().to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(x).float()\n",
    "\n",
    "                correct += (pred.argmax(dim=1) == y).sum().item()\n",
    "                total += y.shape[0]\n",
    "\n",
    "            logger.add_scalar(\"accuracy\", correct / total, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eabb2170a5867983\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eabb2170a5867983\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6005;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Start tensorboard\n",
    "\n",
    "shutil.rmtree(tempfile.gettempdir() + \"/.tensorboard-info\", ignore_errors=True) # sort of 'force reload' for tensorboard\n",
    "%load_ext tensorboard \n",
    "%tensorboard --logdir log_dir --reload_interval 1 --port 6005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
