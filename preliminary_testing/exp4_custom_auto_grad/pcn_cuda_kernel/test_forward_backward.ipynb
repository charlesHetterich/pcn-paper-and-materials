{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import copy \n",
    "from time import time\n",
    "\n",
    "import pcn_kernels\n",
    "\n",
    "class pcnpass(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(c: torch.Tensor, l: torch.Tensor, lnext: torch.Tensor, b: torch.Tensor):\n",
    "        output = pcn_kernels.forward(c, l, lnext, b)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, _):\n",
    "        c, l, lnext, b = inputs\n",
    "        ctx.save_for_backward(c, l, lnext, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_cnext):\n",
    "        c, l, lnext, b = ctx.saved_tensors\n",
    "        grad_c, grad_l, grad_lnext, grad_b = pcn_kernels.backward(grad_cnext.contiguous(), c, l, lnext, b)\n",
    "        return grad_c, grad_l, grad_lnext, grad_b\n",
    "        # return None, None, None, None\n",
    "\n",
    "def tri(period: float, amplitude: float):\n",
    "    \"\"\"\n",
    "    triangle wave function centered around 0 with period and amplitude\n",
    "    \"\"\"\n",
    "\n",
    "    def triangle_wave_transform(x: torch.Tensor):\n",
    "        # using sigal\n",
    "        return (amplitude / period) * (\n",
    "            (period - abs(x % (2 * period) - (1 * period)) - period / 2)\n",
    "        )\n",
    "\n",
    "    return triangle_wave_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0004994869232177734 seconds\n",
      "tensor(0.6575, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Time taken: 0.25350189208984375 seconds\n",
      "Time taken: 3.120999336242676 seconds\n"
     ]
    }
   ],
   "source": [
    "# Speed @ scale test\n",
    "\n",
    "B = 32\n",
    "N = 5000\n",
    "F = 5000\n",
    "D = 16\n",
    "\n",
    "l = nn.Parameter(torch.rand(N, D))\n",
    "lnext = nn.Parameter(torch.rand(F, D))\n",
    "c = nn.Parameter(torch.rand(B, N))\n",
    "b = nn.Parameter(torch.rand(F))\n",
    "# l = torch.rand(N, D, requires_grad=True)\n",
    "# lnext = torch.rand(F, D, requires_grad=True)\n",
    "# c = torch.rand(B, N)\n",
    "# b = torch.rand(F, requires_grad=True)\n",
    "l = l.cuda()\n",
    "lnext = lnext.cuda()\n",
    "c = c.cuda()\n",
    "b = b.cuda()\n",
    "\n",
    "app = pcnpass.apply\n",
    "\n",
    "t = time()\n",
    "cnext: torch.Tensor = app(c, l, lnext, b)\n",
    "# cnext = 1 + (c @ (tri(0.1, 2)(torch.cdist(l, lnext))) / np.sqrt(l.shape[0]) + b)\n",
    "# cnext += 1\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "t = time()\n",
    "print(cnext[0, 0])\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "t = time()\n",
    "cnext.sum().backward()\n",
    "print(f\"Time taken: {time() - t} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7459, 0.5493, 0.5304,  ..., 0.5658, 0.7056, 0.6523],\n",
       "        [0.8264, 0.8070, 0.6457,  ..., 0.3933, 0.2267, 0.8523],\n",
       "        [0.7831, 0.7814, 0.4453,  ..., 0.7721, 0.3657, 0.3119],\n",
       "        ...,\n",
       "        [0.7208, 0.6646, 0.4097,  ..., 0.7346, 0.4220, 0.6039],\n",
       "        [0.6091, 0.8735, 0.5585,  ..., 0.4688, 0.2861, 0.5726],\n",
       "        [0.6846, 0.5720, 0.6047,  ..., 0.3046, 0.1498, 0.5137]],\n",
       "       device='cuda:0', grad_fn=<pcnpassBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.3254985809326172 seconds\n",
      "Time taken: 0.0020017623901367188 seconds\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "N = 500\n",
    "F = 5000\n",
    "D = 64\n",
    "\n",
    "l = torch.rand(N, D, requires_grad=True)\n",
    "lnext = torch.rand(F, D, requires_grad=True)\n",
    "c = torch.rand(B, N, requires_grad=True)\n",
    "b = torch.rand(F, requires_grad=True)\n",
    "\n",
    "l2 = copy.deepcopy(l)\n",
    "lnext2 = copy.deepcopy(lnext)\n",
    "c2 = copy.deepcopy(c)\n",
    "b2 = copy.deepcopy(b)\n",
    "\n",
    "l = l.cuda()\n",
    "lnext = lnext.cuda()\n",
    "c = c.cuda()\n",
    "b = b.cuda()\n",
    "l2 = l2.cuda()\n",
    "lnext2 = lnext2.cuda()\n",
    "c2 = c2.cuda()\n",
    "b2 = b2.cuda()\n",
    "\n",
    "l.retain_grad()\n",
    "lnext.retain_grad()\n",
    "c.retain_grad()\n",
    "b.retain_grad()\n",
    "l2.retain_grad()\n",
    "lnext2.retain_grad()\n",
    "c2.retain_grad()\n",
    "b2.retain_grad()\n",
    "\n",
    "cnext: torch.Tensor = pcnpass.apply(c, l, lnext, b)\n",
    "cnext2: torch.Tensor = c2 @ ((tri(0.1, 2)(torch.cdist(l2, lnext2))) / np.sqrt(l2.shape[0])) + b2\n",
    "\n",
    "t = time()\n",
    "cnext.mean().backward()\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "t = time()\n",
    "cnext2.mean().backward()\n",
    "print(f\"Time taken: {time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[ 8.9799e-04,  1.4514e-03,  6.9525e-04,  ...,  1.3024e-03,\n",
      "          3.1200e-05, -8.5248e-04],\n",
      "        [ 6.5266e-04, -3.9078e-04,  2.9419e-04,  ...,  2.9573e-04,\n",
      "          4.9298e-05, -6.1420e-04],\n",
      "        [ 2.5319e-04,  5.6479e-04, -1.4843e-05,  ...,  9.6867e-04,\n",
      "         -4.6092e-04,  1.5381e-04],\n",
      "        ...,\n",
      "        [-7.5668e-04,  8.4798e-04, -7.0788e-04,  ...,  7.9027e-04,\n",
      "          6.7182e-04,  8.4375e-05],\n",
      "        [-9.6641e-04, -1.9852e-04,  4.2675e-05,  ..., -2.8976e-04,\n",
      "          1.4574e-03,  1.6807e-04],\n",
      "        [-5.5577e-05, -5.8934e-04,  5.6020e-04,  ..., -7.7825e-04,\n",
      "         -9.7816e-04, -1.2462e-03]], device='cuda:0')\n",
      "tensor([[ 8.9799e-04,  1.4514e-03,  6.9525e-04,  ...,  1.3024e-03,\n",
      "          3.1200e-05, -8.5248e-04],\n",
      "        [ 6.1246e-04, -4.1406e-04,  2.7982e-04,  ...,  2.6567e-04,\n",
      "          5.0411e-05, -6.0400e-04],\n",
      "        [ 2.5319e-04,  5.6478e-04, -1.4840e-05,  ...,  9.6867e-04,\n",
      "         -4.6091e-04,  1.5381e-04],\n",
      "        ...,\n",
      "        [-7.5669e-04,  8.4797e-04, -7.0789e-04,  ...,  7.9026e-04,\n",
      "          6.7182e-04,  8.4373e-05],\n",
      "        [-9.7578e-04, -1.8971e-04,  3.3672e-05,  ..., -3.3825e-04,\n",
      "          1.4502e-03,  1.5263e-04],\n",
      "        [-5.5576e-05, -5.8935e-04,  5.6020e-04,  ..., -7.7825e-04,\n",
      "         -9.7816e-04, -1.2462e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "v1 = l.grad\n",
    "v2 = l2.grad\n",
    "\n",
    "print(torch.allclose(v1, v2))\n",
    "print(v1)\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8281e-05, device='cuda:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cnext - cnext2).abs().max()\n",
    "# (cnext - cnext2).var()\n",
    "# cnext, cnext2\n",
    "# cnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(3.8281e-05, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(8.3901e-10, device='cuda:0'),\n",
       " tensor(5.7835e-05, device='cuda:0'),\n",
       " tensor(5.4964e-05, device='cuda:0'),\n",
       " tensor(1.4552e-11, device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.allclose(cnext, cnext2, atol=1e-6)),\n",
    "print(torch.allclose(c.grad, c2.grad, atol=1e-6)),\n",
    "print(torch.allclose(l.grad, l2.grad, atol=1e-6)),\n",
    "print(torch.allclose(lnext.grad, lnext2.grad, atol=1e-6)),\n",
    "print(torch.allclose(b.grad, b2.grad))\n",
    "\n",
    "(cnext - cnext2).max(), \\\n",
    "(c.grad - c2.grad).max(), \\\n",
    "(l.grad - l2.grad).max(), \\\n",
    "(lnext.grad - lnext2.grad).max(), \\\n",
    "(b.grad - b2.grad).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward time:  0.0\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                         model_backward         0.01%       1.678ms        50.00%        9.299s        9.299s        8.228s        50.00%        8.228s        8.228s             1  \n",
      "autograd::engine::evaluate_function: pcnpassBackward...         0.00%      36.000us         0.00%     880.000us     880.000us       4.000us         0.00%        8.228s        8.228s             1  \n",
      "                                        pcnpassBackward         0.00%     725.000us         0.00%     844.000us     844.000us        8.227s        49.99%        8.228s        8.228s             1  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         0.00%      84.000us        49.99%        9.298s        2.324s      15.000us         0.00%     580.000us     145.000us             4  \n",
      "                                        ToCopyBackward0         0.00%      14.000us        49.99%        9.298s        2.324s       9.000us         0.00%     565.000us     141.250us             4  \n",
      "autograd::engine::evaluate_function: torch::autograd...        49.99%        9.298s        49.99%        9.298s        2.324s      15.000us         0.00%     559.000us     139.750us             4  \n",
      "                                               aten::to         0.00%      20.000us        49.99%        9.298s        9.298s       4.000us         0.00%     556.000us     556.000us             1  \n",
      "                                         aten::_to_copy         0.00%      58.000us        49.99%        9.298s        9.298s       8.000us         0.00%     552.000us     552.000us             1  \n",
      "                        torch::autograd::AccumulateGrad         0.00%      12.000us         0.00%      24.000us       6.000us     540.000us         0.00%     544.000us     136.000us             4  \n",
      "                                            aten::copy_        49.99%        9.298s        49.99%        9.298s        9.298s     541.000us         0.00%     541.000us     541.000us             1  \n",
      "     autograd::engine::evaluate_function: MeanBackward0         0.00%      34.000us         0.00%     177.000us     177.000us       3.000us         0.00%     166.000us     166.000us             1  \n",
      "                                          MeanBackward0         0.00%      39.000us         0.00%     143.000us     143.000us       7.000us         0.00%     163.000us     163.000us             1  \n",
      "                                              aten::div         0.00%      76.000us         0.00%      76.000us      76.000us     152.000us         0.00%     152.000us     152.000us             1  \n",
      "                                            aten::zeros         0.00%      49.000us         0.00%     119.000us      29.750us      20.000us         0.00%     128.000us      32.000us             4  \n",
      "                                            aten::zero_         0.00%      26.000us         0.00%      53.000us      13.250us      12.000us         0.00%     102.000us      25.500us             4  \n",
      "                                            aten::fill_         0.00%      63.000us         0.00%      63.000us      12.600us      93.000us         0.00%      93.000us      18.600us             5  \n",
      "                                        aten::ones_like         0.00%      17.000us         0.00%      83.000us      83.000us       5.000us         0.00%      12.000us      12.000us             1  \n",
      "                                            aten::empty         0.00%      17.000us         0.00%      17.000us       4.250us       6.000us         0.00%       6.000us       1.500us             4  \n",
      "                                    aten::empty_strided         0.00%      22.000us         0.00%      22.000us      11.000us       5.000us         0.00%       5.000us       2.500us             2  \n",
      "                                       aten::empty_like         0.00%      14.000us         0.00%      30.000us      30.000us       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                                           aten::expand         0.00%      22.000us         0.00%      28.000us      28.000us       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                                           aten::detach         0.00%       4.000us         0.00%      12.000us      12.000us       2.000us         0.00%       4.000us       4.000us             1  \n",
      "                                       aten::as_strided         0.00%       6.000us         0.00%       6.000us       6.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "                                                 detach         0.00%       8.000us         0.00%       8.000us       8.000us       2.000us         0.00%       2.000us       2.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 18.598s\n",
      "Self CUDA time total: 16.457s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function\n",
    "\n",
    "loss = cnext.mean()\n",
    "\n",
    "with profile(activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_backward\"):\n",
    "        loss.backward()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySin(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(inp, inp2, inp3, inp4):\n",
    "        \"\"\" compute forward pass of custom function \"\"\"\n",
    "        dum1 = torch.zeros_like(inp)\n",
    "        dum2 = torch.zeros_like(inp2)\n",
    "        dum3 = torch.zeros_like(inp3)\n",
    "        dum4 = torch.zeros_like(inp4)\n",
    "        return pcn_kernels.forward(inp, inp2, inp3, inp4)\n",
    "        # pcn_kernels.forward(dum1, dum2, dum3, dum4)\n",
    "        # return torch.zeros((inp.shape[0], inp3.shape[0]), requires_grad=True).cuda()\n",
    "        # return pcn_kernels.forward(inp, inp2, inp3, inp4)\n",
    "        # return inp#(inp + inp2 + inp3 + inp4).sin()  # compute forward pass, can also be computed by any other library\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, _):\n",
    "        inp, inp2, inp3, inp4 = inputs\n",
    "        ctx.save_for_backward(inp, inp2, inp3, inp4)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        \"\"\" compute product of output gradient with the \n",
    "        jacobian of your function evaluated at input \"\"\"\n",
    "        inp, inp2, inp3, inp4 = ctx.saved_tensors\n",
    "        grad_inp = torch.zeros_like(inp)\n",
    "        grad_inp2 = torch.zeros_like(inp2)\n",
    "        grad_inp3 = torch.zeros_like(inp3)\n",
    "        grad_inp4 = torch.zeros_like(inp4)\n",
    "        # print (pcn_kernels.backward(grad_out.contiguous(), inp, inp2, inp3, inp4))\n",
    "        return tuple(pcn_kernels.backward(grad_out.contiguous(), inp, inp2, inp3, inp4))\n",
    "        # grad_inp = grad_out * torch.cos(inp)  # propagate gradient, can also be computed by any other library\n",
    "        return grad_inp, grad_inp2, grad_inp3, grad_inp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.0 seconds\n",
      "Time taken: 0.0010004043579101562 seconds\n",
      "Time taken: 8.629997491836548 seconds\n"
     ]
    }
   ],
   "source": [
    "B = 32\n",
    "N = 5000\n",
    "F = 50000\n",
    "D = 16\n",
    "\n",
    "c = nn.Parameter(torch.rand(B, N))\n",
    "l = nn.Parameter(torch.rand(N, D))\n",
    "lnext = nn.Parameter(torch.rand(F, D))\n",
    "b = nn.Parameter(torch.rand(F))\n",
    "\n",
    "inp1 = nn.Parameter(torch.rand(B, N)).cuda()\n",
    "inp2 = nn.Parameter(torch.rand(N, D)).cuda()\n",
    "inp3 = nn.Parameter(torch.rand(F, D)).cuda()\n",
    "inp4 = nn.Parameter(torch.rand(F)).cuda()\n",
    "\n",
    "t = time()\n",
    "# torch.sin(input + inp2 + inp3 + inp4).sum().backward()\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "t = time()\n",
    "x = MySin.apply(inp1, inp2, inp3, inp4)\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "t = time()\n",
    "x.sum().backward()\n",
    "print(f\"Time taken: {time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a mystery to me what is happening with the inner mechanics of the backward pass\n",
    "# the `pcn_kernel.backward` runs quickly like the forward pass, and has printable values\n",
    "# but timing the full `.backward()` is very slow\n",
    "\n",
    "# The pcn_kernel.backward doesn't even need to be called in the backward pass\n",
    "# regardless of what happens in the backward pass, if the forward pass calls `pcn_kernel.forward`\n",
    "# then the backward pass ends up being much slower then if that isn't called in the forward pass\n",
    "# Why is the backward time affected by what happens in the forward pass ???????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.024500370025634766 seconds\n",
      "Time taken: 0.0 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pcn_kernels\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "B = 32\n",
    "N = 50000\n",
    "F = 50000\n",
    "D = 16\n",
    "\n",
    "t = time()\n",
    "c = nn.Parameter(torch.rand(B, N)).cuda()\n",
    "l = nn.Parameter(torch.rand(N, D)).cuda()\n",
    "lnext = nn.Parameter(torch.rand(F, D)).cuda()\n",
    "b = nn.Parameter(torch.rand(F)).cuda()\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "t = time()\n",
    "cnext = pcn_kernels.forward(c, l, lnext, b)\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "# sum = cnext.sum()\n",
    "# # manually calculate grad_cnext\n",
    "# grad_cnext = torch.Tensor(cnext.shape).cuda()\n",
    "# grad_cnext.fill_(1.0 / np.sqrt(cnext.shape[0]))\n",
    "\n",
    "# t = time()\n",
    "# grad_c, grad_l, grad_lnext, grad_b = pcn_kernels.backward(grad_cnext, c, l, lnext, b)\n",
    "# print(f\"Time taken: {time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2098, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnext[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3853,  0.7242,  0.3629,  ...,  0.4866,  0.5627,  0.0840],\n",
      "        [ 0.6922,  0.6477,  0.1949,  ...,  0.3359,  0.5440,  0.1395],\n",
      "        [ 0.4976,  0.4154,  0.4153,  ...,  0.4049,  0.4551, -0.0340],\n",
      "        ...,\n",
      "        [ 0.7102,  0.9383,  0.4187,  ...,  0.3982,  0.3767, -0.2267],\n",
      "        [ 0.4361,  0.7735,  0.3790,  ...,  0.3035,  0.5073, -0.1499],\n",
      "        [ 0.5498,  0.7083,  0.3224,  ...,  0.5351,  0.6693, -0.0820]],\n",
      "       device='cuda:0')\n",
      "Time taken: 9.345498323440552 seconds\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "print(cnext)\n",
    "print(f\"Time taken: {time() - t} seconds\")\n",
    "\n",
    "# It seems to maybe be the case that kernel function outputs are lazy loaded?\n",
    "# - If we run the above code, and above block and then this block, this block takes ~7.8 seconds\n",
    "# - If we then run the top block again, the first time taken will be low (~0.01 seconds)\n",
    "# - But if we run the top block twice in a row, the first time taken will be high (~7.8 seconds)\n",
    "# - whichs makes me think that, here, by printing one of the grads, we force the `pcn_kernel.backward` to trigger\n",
    "# - In the first time taken section in the above block, we trigger the `pcn_kernel.backward` pass\n",
    "#   by trying to reset some of the inputs of the `pcn_kernel.backward` to new values\n",
    "\n",
    "# So the moral of the story is, my backward pass simply is just slow. I should be able to fix this with a better\n",
    "# threading layout and hopefully removing the need for atomic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6530, 14210, 15674)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pcn2d\n",
    "# pcn16d\n",
    "(784 + 512 + 10) * 5, \\\n",
    "(784 + 2048 + 10) * 5, \\\n",
    "(784 + 128 + 10) * 17,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7960, 50890, 101770, 134794)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xs\n",
    "# s\n",
    "# m\n",
    "# l\n",
    "((784 + 1) * 10 + (10 + 1) * 10), \\\n",
    "((784 + 1) * 64 + (64 + 1) * 10), \\\n",
    "((784 + 1) * 128 + (128 + 1) * 10), \\\n",
    "((784 + 1) * 128 + (128 + 1) * 128 + (128 + 1) * 128 + (128 + 1) * 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
