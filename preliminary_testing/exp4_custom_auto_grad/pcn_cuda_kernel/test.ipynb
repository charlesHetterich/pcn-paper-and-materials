{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pcn_kernels\n",
    "\n",
    "class pcnpass(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(c: torch.Tensor, l: torch.Tensor, lnext: torch.Tensor, b: torch.Tensor):\n",
    "        output = pcn_kernels.forward(c, l, lnext, b)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, _):\n",
    "        c, l, lnext, b = inputs\n",
    "        ctx.save_for_backward(c, l, lnext, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_cnext):\n",
    "        c, l, lnext, b = ctx.saved_tensors\n",
    "        grad_c, grad_l, grad_lnext, grad_b = pcn_kernels.backward(grad_cnext.contiguous(), c, l, lnext, b)\n",
    "        return grad_c, grad_l, grad_lnext, grad_b\n",
    "\n",
    "\n",
    "class PCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: list[int],\n",
    "        dimensions: int = 20,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if len(layers) < 2:\n",
    "            raise ValueError(\"At least 2 layers are required\")\n",
    "\n",
    "        self.layers = nn.ParameterList(\n",
    "            [nn.Parameter(torch.rand(l, dimensions) * 2 - 1) for l in layers]\n",
    "        )\n",
    "        self.layers_bias = nn.ParameterList(\n",
    "            [nn.Parameter((torch.rand(l) * 2 - 1) * 0.1) for l in layers]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = x\n",
    "        for i, (l, lnext) in enumerate(zip(self.layers, self.layers[1:])):\n",
    "            z = pcnpass.apply(z, l, lnext, self.layers_bias[i + 1])\n",
    "            if i < len(self.layers) - 2:\n",
    "                z = torch.relu(z)\n",
    "        return z\n",
    "\n",
    "# control model\n",
    "class FCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_in: int,\n",
    "        n_out: int,\n",
    "        hidden: list[int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        c = n_in\n",
    "        L = []\n",
    "        for l in hidden:\n",
    "            L.append(nn.Linear(c, l))\n",
    "            L.append(nn.ReLU())\n",
    "            c = l\n",
    "\n",
    "        L.append(nn.Linear(l, n_out))\n",
    "        self.net = nn.Sequential(*L)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3311,  0.1757, -0.4517,  ...,  0.0432,  0.0869, -0.0188],\n",
       "        [ 0.1551,  0.1073, -0.0605,  ..., -0.2180,  0.1503,  0.0268],\n",
       "        [ 0.0686,  0.2837, -0.3161,  ...,  0.3544,  0.0527,  0.0246],\n",
       "        ...,\n",
       "        [ 0.1341, -0.0568, -0.3880,  ...,  0.1064,  0.0698, -0.1622],\n",
       "        [ 0.0499,  0.1083, -0.0751,  ...,  0.0617,  0.4562, -0.4796],\n",
       "        [ 0.1600,  0.2338, -0.4881,  ..., -0.0972, -0.2094, -0.1343]],\n",
       "       device='cuda:0', grad_fn=<pcnpassBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "in_size = 1000\n",
    "out_size = 100\n",
    "\n",
    "num_tests = 100\n",
    "x = torch.rand(num_tests, in_size).to(gpu)\n",
    "pcn = PCN([in_size, out_size]).to(gpu)\n",
    "pcn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-5.5202e-05, device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# {layer size} -> {output variance} analysis\n",
    "\n",
    "\n",
    "in_size = 1000\n",
    "out_size = 100\n",
    "\n",
    "num_tests = 100\n",
    "agg_var = []\n",
    "agg_mean = []\n",
    "for i in range(num_tests):\n",
    "    pcn = PCN([in_size, in_size, in_size, in_size, out_size]).to(gpu)\n",
    "    agg_var.append(pcn(torch.rand(1, in_size).to(gpu)).var())\n",
    "    agg_mean.append(pcn(torch.rand(1, in_size).to(gpu)).mean())\n",
    "\n",
    "torch.stack(agg_mean).mean(), torch.stack(agg_var).mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
