{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pcn_kernels\n",
    "class pcnpass(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(c: torch.Tensor, l: torch.Tensor, lnext: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "        output: torch.Tensor = pcn_kernels.forward(c, l, lnext, b)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, _):\n",
    "        c, l, lnext, b = inputs\n",
    "        ctx.save_for_backward(c, l, lnext, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_cnext) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        c, l, lnext, b = ctx.saved_tensors\n",
    "        grad_c, grad_l, grad_lnext, grad_b = pcn_kernels.backward(grad_cnext.contiguous(), c, l, lnext, b)\n",
    "        return grad_c, grad_l, grad_lnext, grad_b\n",
    "\n",
    "FREQ = 10.0\n",
    "\n",
    "# def tri(z):\n",
    "#     return 1 - 2 * abs(((z) % 2) - 1)\n",
    "\n",
    "def tri_new_derivative(z):\n",
    "    return 2 if z % 2 < 1 else -2\n",
    "\n",
    "def tri(period: float, amplitude: float):\n",
    "    \"\"\"\n",
    "    triangle wave function centered around 0 with period and amplitude\n",
    "    \"\"\"\n",
    "\n",
    "    def triangle_wave_transform(x: torch.Tensor):\n",
    "        # using sigal\n",
    "        return (amplitude / period) * (\n",
    "            (period - abs(x % (2 * period) - (1 * period)) - period / 2)\n",
    "        )\n",
    "\n",
    "    return triangle_wave_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "import numpy as np\n",
    "from time import time\n",
    "import copy\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "B = 32\n",
    "N = 500\n",
    "F = 50000\n",
    "D = 4\n",
    "\n",
    "l = torch.rand(N, D, requires_grad=True)\n",
    "lnext = torch.rand(F, D, requires_grad=True)\n",
    "c = torch.rand(B, N, requires_grad=True)\n",
    "b = torch.rand(F, requires_grad=True)\n",
    "\n",
    "\n",
    "# all to gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038500070571899414\n",
      "0.20549821853637695\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# if N * F < 1e8:\n",
    "l2 = copy.deepcopy(l)\n",
    "lnext2 = copy.deepcopy(lnext)\n",
    "c2 = copy.deepcopy(c)\n",
    "b2 = copy.deepcopy(b)\n",
    "\n",
    "l = l.to(gpu)\n",
    "lnext = lnext.to(gpu)\n",
    "c = c.to(gpu)\n",
    "b = b.to(gpu)\n",
    "l2 = l2.to(gpu)\n",
    "lnext2 = lnext2.to(gpu)\n",
    "c2 = c2.to(gpu)\n",
    "b2 = b2.to(gpu)\n",
    "l2.retain_grad()\n",
    "lnext2.retain_grad()\n",
    "c2.retain_grad()\n",
    "b2.retain_grad()\n",
    "# Retain grad\n",
    "l.retain_grad()\n",
    "lnext.retain_grad()\n",
    "c.retain_grad()\n",
    "b.retain_grad()\n",
    "t = time()\n",
    "cnext: torch.Tensor = pcnpass.apply(c, l, lnext, b)\n",
    "print(time() - t)\n",
    "cnext.retain_grad()\n",
    "\n",
    "cnext2 = None\n",
    "if N * F < 1e8:\n",
    "    t = time()\n",
    "    cnext2 = (c2 @ (tri(0.1, 1)(torch.cdist(l2, lnext2)) / np.sqrt(l2.shape[0]))) + b2\n",
    "    print(time() - t)\n",
    "    cnext2.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0156, 0.0041, 0.0038,  ..., 0.0076,    nan, 0.0049],\n",
       "        [0.0769, 0.0054, 0.0036,  ..., 0.0069,    nan, 0.0044],\n",
       "        [0.0248, 0.0050, 0.0033,  ..., 0.0062,    nan, 0.0063],\n",
       "        ...,\n",
       "        [0.0354, 0.0050, 0.0034,  ..., 0.0059,    nan, 0.0038],\n",
       "        [0.0185, 0.0068, 0.0037,  ..., 0.0076,    nan, 0.0061],\n",
       "        [0.0151, 0.0046, 0.0039,  ..., 0.0056,    nan, 0.0040]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnext2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5574979782104492\n",
      "0.04899883270263672\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum(cnext ** (1/10)) / B\n",
    "\n",
    "t = time()\n",
    "loss.backward()\n",
    "print(time() - t)\n",
    "\n",
    "if cnext2 is not None:\n",
    "    loss2 = torch.sum(cnext2 ** (1/10)) / B\n",
    "    t = time()\n",
    "    loss2.backward()\n",
    "    print(time() - t)\n",
    "\n",
    "    # Report findings\n",
    "    print(torch.allclose(c.grad, c2.grad))\n",
    "    print(torch.allclose(l.grad, l2.grad))\n",
    "    print(torch.allclose(lnext.grad, lnext2.grad))\n",
    "    print(torch.allclose(b.grad, b2.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2256,  0.2692,  0.4401,  0.5024],\n",
      "        [    nan,     nan,     nan,     nan],\n",
      "        [ 0.0703,  2.0552, -1.9464, -0.9433],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,     nan],\n",
      "        [-0.7863,  0.0325, -0.0623, -0.4276],\n",
      "        [ 0.1722, -0.6660, -0.0199, -0.1288]], device='cuda:0')\n",
      "tensor([[-0.0827,  0.1060,  0.1561,  0.1937],\n",
      "        [    nan,     nan,     nan,     nan],\n",
      "        [ 0.0281,  0.8795, -0.8420, -0.3483],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,     nan],\n",
      "        [-0.4081,  0.0196, -0.0338, -0.2233],\n",
      "        [ 0.0872, -0.3473, -0.0112, -0.0654]], device='cuda:0')\n",
      "tensor([[ 0.1430, -0.1633, -0.2840, -0.3087],\n",
      "        [    nan,     nan,     nan,     nan],\n",
      "        [-0.0422, -1.1757,  1.1044,  0.5950],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,     nan],\n",
      "        [ 0.3782, -0.0129,  0.0284,  0.2043],\n",
      "        [-0.0850,  0.3187,  0.0087,  0.0634]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "v = lnext\n",
    "v2 = lnext2\n",
    "\n",
    "print(v.grad)\n",
    "print(v2.grad)\n",
    "print(v2.grad - v.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan, device='cuda:0'),\n",
       " tensor(nan, device='cuda:0'),\n",
       " tensor(nan, device='cuda:0'),\n",
       " tensor(nan, device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c.grad - c2.grad).max(), \\\n",
    "(l.grad - l2.grad).max(), \\\n",
    "(lnext.grad - lnext2.grad).max(), \\\n",
    "(b.grad - b2.grad).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0004, device='cuda:0'),\n",
       " tensor(6.4654e-05, device='cuda:0'),\n",
       " tensor(0.0602, device='cuda:0'),\n",
       " tensor(0.0029, device='cuda:0'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(c.grad - c2.grad).max() / c.grad.var(), \\\n",
    "(l.grad - l2.grad).max() / l.grad.var(), \\\n",
    "(lnext.grad - lnext2.grad).max() / lnext.grad.var(), \\\n",
    "(b.grad - b2.grad).max() / b.grad.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0007, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lnext.grad - lnext2.grad).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale test\n",
    "\n",
    "from time import time\n",
    "import torch\n",
    "gpu = torch.device('cuda:0')\n",
    "\n",
    "B = 64\n",
    "N = 5000\n",
    "F = 500000\n",
    "D = 4\n",
    "\n",
    "l = torch.rand(N, D, requires_grad=True).to(gpu)\n",
    "lnext = torch.rand(F, D, requires_grad=True).to(gpu)\n",
    "c = torch.rand(B, N, requires_grad=True).to(gpu)\n",
    "b = torch.rand(F, requires_grad=True).to(gpu)\n",
    "\n",
    "l.retain_grad()\n",
    "lnext.retain_grad()\n",
    "c.retain_grad()\n",
    "b.retain_grad()\n",
    "\n",
    "t = time()\n",
    "cnext = pcnpass.apply(c, l, lnext, b)\n",
    "print(time() - t)\n",
    "cnext.retain_grad()\n",
    "\n",
    "loss = torch.sum(cnext)\n",
    "\n",
    "t = time()\n",
    "loss.backward()\n",
    "print(time() - t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
