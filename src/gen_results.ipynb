{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import util\n",
    "import datasets\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_names = [\"cifar10\", \"cifar100\"]\n",
    "models_folder = \"../trained_models\"\n",
    "\n",
    "def run_epoch(model, train, test, device, in_features, flatten = False):\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    agg_acc_top1 = []\n",
    "    agg_loss = []\n",
    "    model.train()\n",
    "    with torch.no_grad():\n",
    "        for x, y in train:\n",
    "            if flatten:\n",
    "                x = x.view(-1, in_features)\n",
    "            x, y = x.to(device).float(), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss_value = criterion(pred, y)\n",
    "            agg_acc_top1.append((pred.cpu().argmax(1) == y.cpu()).float().mean())\n",
    "            agg_loss.append(loss_value.item())\n",
    "        top1_train = round(np.mean(agg_acc_top1) * 100, 1)\n",
    "        loss = round(np.mean(agg_loss), 4)\n",
    "\n",
    "    agg_acc_top1 = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test:\n",
    "            if flatten:\n",
    "                x = x.view(-1, in_features)\n",
    "            x, y = x.to(device).float(), y.to(device)\n",
    "            pred = model(x)\n",
    "            agg_acc_top1.append((pred.cpu().argmax(1) == y.cpu()).float().mean())\n",
    "        top1_test = round(np.mean(agg_acc_top1) * 100, 1)\n",
    "\n",
    "    return {\n",
    "        'Loss': round(loss, 3),\n",
    "        'Top-1 Accuracy (train)': top1_train,\n",
    "        'Top-1 Accuracy (test)': top1_test,\n",
    "    }\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, conv_layers: list[int], linear_layers: list[int], linear_type: str, pcn_dims: int = None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert len(conv_layers) >= 2, \"Must be at least 2 convolutional layers\"\n",
    "        assert len(linear_layers) >= 1, \"Must be at least 1 linear layer\"\n",
    "        assert linear_type in [\"mlp\", \"pcn\"], \"linear_type must be either 'mlp' or 'pcn'\"\n",
    "        if linear_type == \"pcn\":\n",
    "            assert pcn_dims is not None, \"pcn_dims must be specified if linear_type is 'pcn'\"\n",
    "            \n",
    "        L = [\n",
    "            torch.nn.Conv2d(conv_layers[0], conv_layers[1], 7, padding=3, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "        ]\n",
    "        c = conv_layers[1]\n",
    "        for l in conv_layers[2:]:\n",
    "            L.append(torch.nn.Conv2d(c, l, 3, padding=1, stride=2))\n",
    "            L.append(torch.nn.ReLU())\n",
    "            c = l\n",
    "        self.net = torch.nn.Sequential(*L)\n",
    "        self.classifier = util.MLP([c, *linear_layers]) if linear_type == \"mlp\" \\\n",
    "            else util.PCN([c, *linear_layers], pcn_dims)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.net(x)\n",
    "        z = torch.amax(z, dim=[2, 3])\n",
    "        return self.classifier(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hette\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCN$_{4}$ on cifar10...\n",
      "Running PCN$_{8}$ on cifar10...\n",
      "Running PCN$_{16}$ on cifar10...\n",
      "Running PCN$_{32}$ on cifar10...\n",
      "Running MLP on cifar10...\n",
      "Running PCN$_{4}$ on cifar100...\n",
      "Running PCN$_{8}$ on cifar100...\n",
      "Running PCN$_{16}$ on cifar100...\n",
      "Running PCN$_{32}$ on cifar100...\n",
      "Running MLP on cifar100...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LINEARNET\n",
    "\n",
    "model_info = [\n",
    "    (4, \"LinearNet_pcn4.pt\", \"PCN$_{4}$\"),\n",
    "    (8, \"LinearNet_pcn8.pt\", \"PCN$_{8}$\"),\n",
    "    (16, \"LinearNet_pcn16.pt\", \"PCN$_{16}$\"),\n",
    "    (32, \"LinearNet_pcn32.pt\", \"PCN$_{32}$\"),\n",
    "    (None, \"LinearNet_mlp.pt\", \"MLP\"),\n",
    "]\n",
    "\n",
    "model_statistics: list[dict] = []\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    batch_size = 64\n",
    "    in_features = 16 * 16 * 3\n",
    "    out_features = 10 if ds_name == \"cifar10\" else 100\n",
    "\n",
    "    train, test = None, None\n",
    "    if ds_name == \"cifar10\":\n",
    "        train = datasets.load_data(datasets.Cifar10(\"train\", size=16), batch_size=batch_size)\n",
    "        test = datasets.load_data(datasets.Cifar10(\"test\", size=16), batch_size=batch_size)\n",
    "    else:\n",
    "        train = datasets.load_data(datasets.Cifar100(\"train\", size=16), batch_size=batch_size)\n",
    "        test = datasets.load_data(datasets.Cifar100(\"test\", size=16), batch_size=batch_size)\n",
    "    \n",
    "\n",
    "    # load models from models_folder folder\n",
    "    model_shape = [in_features, 1024, 1024, 2048, out_features]\n",
    "    for d, fn, name in model_info:\n",
    "        if \"mlp\" in fn:\n",
    "            model = util.MLP(\n",
    "                layers=model_shape,\n",
    "            ).to(device)\n",
    "        elif \"pcn\" in fn:\n",
    "            model = util.PCN(\n",
    "                layers=model_shape,\n",
    "                dimensions=d,\n",
    "            ).to(device)\n",
    "        model.load_state_dict(torch.load(f\"{models_folder}/{ds_name}/\" + fn))\n",
    "\n",
    "        print(f\"Running {name} on {ds_name}...\")\n",
    "\n",
    "        model_statistics.append(\n",
    "            {\n",
    "                \"Model Class\": \"LinearNet\",\n",
    "                \"Dataset\": ds_name,\n",
    "                \"Model\": name,\n",
    "                \"# Convolutional Params (millions)\": None,\n",
    "                \"# Linear Params (millions)\": round(count_params(model) / 1e6, 3),\n",
    "            } | run_epoch(model, train, test, device, in_features, flatten=True)\n",
    "        )\n",
    "\n",
    "df_linear = df = pd.DataFrame(model_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCN$_{16}$ on cifar10...\n",
      "Running MLP on cifar10...\n",
      "Running PCN$_{16}$ on cifar100...\n",
      "Running MLP on cifar100...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CONVNET\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_names = [\"cifar10\", \"cifar100\"]\n",
    "model_info = [\n",
    "    (16, \"ConvNet_pcn16.pt\", \"PCN$_{16}$\"),\n",
    "    (None, \"ConvNet_mlp.pt\", \"MLP\"),\n",
    "]\n",
    "\n",
    "model_statistics: list[dict] = []\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    batch_size = 64\n",
    "    in_features = 3\n",
    "    out_features = 10 if ds_name == \"cifar10\" else 100\n",
    "\n",
    "    train, test = None, None\n",
    "    if ds_name == \"cifar10\":\n",
    "        train = datasets.load_data(datasets.Cifar10(\"train\"), batch_size=batch_size)\n",
    "        test = datasets.load_data(datasets.Cifar10(\"test\"), batch_size=batch_size)\n",
    "    else:\n",
    "        train = datasets.load_data(datasets.Cifar100(\"train\"), batch_size=batch_size)\n",
    "        test = datasets.load_data(datasets.Cifar100(\"test\"), batch_size=batch_size)\n",
    "\n",
    "    # load models from models_folder folder\n",
    "    conv_shape = [in_features, 32, 128, 512, 1024]\n",
    "    linear_shape = [1024, out_features]\n",
    "    for d, fn, name in model_info:\n",
    "        if \"mlp\" in fn:\n",
    "            model = ConvNet(\n",
    "                conv_layers=conv_shape,\n",
    "                linear_layers=linear_shape,\n",
    "                linear_type=\"mlp\"\n",
    "            ).to(device)\n",
    "        elif \"pcn\" in fn:\n",
    "            model = ConvNet(\n",
    "                conv_layers=conv_shape,\n",
    "                linear_layers=linear_shape,\n",
    "                linear_type=\"pcn\",\n",
    "                pcn_dims=d\n",
    "            ).to(device)\n",
    "        \n",
    "        failed_load = False\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(f\"{models_folder}/{ds_name}/{fn}\"))\n",
    "        except:\n",
    "            print(f\"Failed to load {models_folder}/{ds_name}/{fn}\")\n",
    "            failed_load = True\n",
    "\n",
    "        \n",
    "        run_results = None\n",
    "        if not failed_load:\n",
    "            print(f\"Running {name} on {ds_name}...\")\n",
    "            run_results = run_epoch(model, train, test, device, in_features)\n",
    "        model_statistics.append(\n",
    "            {\n",
    "                \"Model Class\": \"ConvNet\",\n",
    "                \"Dataset\": ds_name,\n",
    "                \"Model\": name,\n",
    "                \"# Convolutional Params (millions)\": round(count_params(model.net) / 1e6, 3),\n",
    "                \"# Linear Params (millions)\": round(count_params(model.classifier) / 1e6, 3),\n",
    "            } | (run_results if run_results is not None else {})\n",
    "        )\n",
    "\n",
    "df_conv = pd.DataFrame(model_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hette\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Using cache found in C:\\Users\\hette/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\hette\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hette\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCN$_{16}$ on cifar10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hette/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MLP on cifar10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hette/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCN$_{16}$ on cifar100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hette/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MLP on cifar100...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ALEXNET\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_names = [\"cifar10\", \"cifar100\"]\n",
    "model_info = [\n",
    "    (16, \"AlexNet_pcn16.pt\", \"PCN$_{16}$\"),\n",
    "    (None, \"AlexNet_mlp.pt\", \"MLP\"),\n",
    "]\n",
    "\n",
    "model_statistics: list[dict] = []\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    batch_size = 64\n",
    "    in_features = 3\n",
    "    out_features = 10 if ds_name == \"cifar10\" else 100\n",
    "\n",
    "    train, test = None, None\n",
    "    if ds_name == \"cifar10\":\n",
    "        train = datasets.load_data(datasets.Cifar10(\"train\", size=227), batch_size=batch_size)\n",
    "        test = datasets.load_data(datasets.Cifar10(\"test\", size=227), batch_size=batch_size)\n",
    "    else:\n",
    "        train = datasets.load_data(datasets.Cifar100(\"train\", size=227), batch_size=batch_size)\n",
    "        test = datasets.load_data(datasets.Cifar100(\"test\", size=227), batch_size=batch_size)\n",
    "\n",
    "    # load models from models_folder folder\n",
    "    conv_shape = [in_features, 32, 128, 512, 1024]\n",
    "    linear_shape = [1024, out_features]\n",
    "    for d, fn, name in model_info:\n",
    "        if \"mlp\" in fn:\n",
    "            model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to(device)\n",
    "            model.classifier[6] = torch.nn.Linear(4096, out_features).to(device)\n",
    "        elif \"pcn\" in fn:\n",
    "            model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=False).to(device)\n",
    "            model.classifier = util.PCN([9216, 4096, 4096, out_features], dimensions=d, dropout=0.5).to(device)\n",
    "        \n",
    "        failed_load = False\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(f\"{models_folder}/{ds_name}/{fn}\"))\n",
    "        except:\n",
    "            print(f\"Failed to load {models_folder}/{ds_name}/{fn}\")\n",
    "            failed_load = True\n",
    "\n",
    "        \n",
    "        run_results = None\n",
    "        if not failed_load:\n",
    "            print(f\"Running {name} on {ds_name}...\")\n",
    "            run_results = run_epoch(model, train, test, device, in_features)\n",
    "        model_statistics.append(\n",
    "            {\n",
    "                \"Model Class\": \"AlexNet\",\n",
    "                \"Dataset\": ds_name,\n",
    "                \"Model\": name,\n",
    "                \"# Convolutional Params (millions)\": round(count_params(model.features) / 1e6, 3),\n",
    "                \"# Linear Params (millions)\": round(count_params(model.classifier) / 1e6, 3),\n",
    "            } | (run_results if run_results is not None else {})\n",
    "        )\n",
    "\n",
    "df_alex = pd.DataFrame(model_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_linear, df_conv, df_alex]).to_csv('results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
